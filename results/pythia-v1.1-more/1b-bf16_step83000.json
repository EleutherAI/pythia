{
  "results": {
    "logiqa": {
      "acc": 0.22580645161290322,
      "acc_stderr": 0.016399713788445076,
      "acc_norm": 0.28417818740399386,
      "acc_norm_stderr": 0.017690542680190758
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2522816166883963,
      "acc_stderr": 0.01109278905687523,
      "acc_norm": 0.29465449804432853,
      "acc_norm_stderr": 0.011643576764069559
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.029771775228145628,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.03149328104507956
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 4.815340909090909,
      "likelihood_difference_stderr": 2.0303208300751856,
      "pct_stereotype": 0.6363636363636364,
      "pct_stereotype_stderr": 0.15212000482437738
    },
    "hendrycksTest-management": {
      "acc": 0.17475728155339806,
      "acc_stderr": 0.03760178006026623,
      "acc_norm": 0.2524271844660194,
      "acc_norm_stderr": 0.04301250399690876
    },
    "hendrycksTest-world_religions": {
      "acc": 0.30409356725146197,
      "acc_stderr": 0.03528211258245232,
      "acc_norm": 0.38596491228070173,
      "acc_norm_stderr": 0.03733756969066164
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2582781456953642,
      "acc_stderr": 0.035737053147634576,
      "acc_norm": 0.2251655629139073,
      "acc_norm_stderr": 0.03410435282008936
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.4032841435185186,
      "likelihood_difference_stderr": 0.22083516182242693,
      "pct_stereotype": 0.5370370370370371,
      "pct_stereotype_stderr": 0.03400603625538272
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.26145251396648045,
      "acc_stderr": 0.014696599650364553,
      "acc_norm": 0.26927374301675977,
      "acc_norm_stderr": 0.014835616582882578
    },
    "arc_challenge": {
      "acc": 0.23464163822525597,
      "acc_stderr": 0.01238387356076868,
      "acc_norm": 0.2593856655290102,
      "acc_norm_stderr": 0.012808273573927099
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.5991043613707165,
      "likelihood_difference_stderr": 0.1802506132763118,
      "pct_stereotype": 0.5295950155763239,
      "pct_stereotype_stderr": 0.02790184442005117
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.24102564102564103,
      "acc_stderr": 0.021685546665333195,
      "acc_norm": 0.2564102564102564,
      "acc_norm_stderr": 0.02213908110397154
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847415
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.270042194092827,
      "acc_stderr": 0.028900721906293426,
      "acc_norm": 0.29535864978902954,
      "acc_norm_stderr": 0.029696338713422882
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.303980068897638,
      "likelihood_difference_stderr": 0.14594791738068702,
      "pct_stereotype": 0.5295275590551181,
      "pct_stereotype_stderr": 0.022167024359332235
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.23773584905660378,
      "acc_stderr": 0.0261998088075619,
      "acc_norm": 0.30566037735849055,
      "acc_norm_stderr": 0.028353298073322666
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2282958199356913,
      "acc_stderr": 0.02383930331139822,
      "acc_norm": 0.3022508038585209,
      "acc_norm_stderr": 0.026082700695399662
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.5054253472222223,
      "likelihood_difference_stderr": 0.33792577821603964,
      "pct_stereotype": 0.6944444444444444,
      "pct_stereotype_stderr": 0.054668187059789194
    },
    "hendrycksTest-college_physics": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.04220773659171452,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.04533838195929775
    },
    "sciq": {
      "acc": 0.828,
      "acc_stderr": 0.011939788882495321,
      "acc_norm": 0.733,
      "acc_norm_stderr": 0.013996674851796282
    },
    "hendrycksTest-anatomy": {
      "acc": 0.23703703703703705,
      "acc_stderr": 0.03673731683969506,
      "acc_norm": 0.17037037037037037,
      "acc_norm_stderr": 0.03247781185995593
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.2909628378378377,
      "likelihood_difference_stderr": 0.32873655119660133,
      "pct_stereotype": 0.7387387387387387,
      "pct_stereotype_stderr": 0.041887708614324
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3306122448979592,
      "acc_stderr": 0.030116426296540606,
      "acc_norm": 0.2693877551020408,
      "acc_norm_stderr": 0.02840125202902294
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.16,
      "acc_stderr": 0.03684529491774708,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-astronomy": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.03317672787533157,
      "acc_norm": 0.34210526315789475,
      "acc_norm_stderr": 0.03860731599316092
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.017848089574913226,
      "acc_norm": 0.2581699346405229,
      "acc_norm_stderr": 0.017704531653250075
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 4.161830357142857,
      "likelihood_difference_stderr": 0.297648754383943,
      "pct_stereotype": 0.6275510204081632,
      "pct_stereotype_stderr": 0.03462107977939841
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.2503192848020434,
      "acc_stderr": 0.015491088951494583,
      "acc_norm": 0.25287356321839083,
      "acc_norm_stderr": 0.015543377313719681
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.803763440860215,
      "likelihood_difference_stderr": 0.4611301064572897,
      "pct_stereotype": 0.7311827956989247,
      "pct_stereotype_stderr": 0.046221879226940606
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.24277456647398843,
      "acc_stderr": 0.0326926380614177,
      "acc_norm": 0.24855491329479767,
      "acc_norm_stderr": 0.03295304696818318
    },
    "hendrycksTest-prehistory": {
      "acc": 0.24691358024691357,
      "acc_stderr": 0.02399350170904211,
      "acc_norm": 0.21296296296296297,
      "acc_norm_stderr": 0.022779719088733396
    },
    "wsc": {
      "acc": 0.36538461538461536,
      "acc_stderr": 0.0474473339327792
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2,
      "acc_stderr": 0.031234752377721175,
      "acc_norm": 0.296969696969697,
      "acc_norm_stderr": 0.03567969772268048
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2832369942196532,
      "acc_stderr": 0.02425790170532337,
      "acc_norm": 0.3179190751445087,
      "acc_norm_stderr": 0.025070713719153186
    },
    "hendrycksTest-virology": {
      "acc": 0.3253012048192771,
      "acc_stderr": 0.03647168523683228,
      "acc_norm": 0.25903614457831325,
      "acc_norm_stderr": 0.03410646614071856
    },
    "hendrycksTest-computer_security": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.24220183486238533,
      "acc_stderr": 0.01836817630659862,
      "acc_norm": 0.23119266055045873,
      "acc_norm_stderr": 0.01807575024163315
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.022019080012217897,
      "acc_norm": 0.2671957671957672,
      "acc_norm_stderr": 0.02278967314577657
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.23225806451612904,
      "acc_stderr": 0.024022256130308235,
      "acc_norm": 0.2806451612903226,
      "acc_norm_stderr": 0.025560604721022895
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.7914523703041145,
      "likelihood_difference_stderr": 0.0924737279603875,
      "pct_stereotype": 0.49970184853905786,
      "pct_stereotype_stderr": 0.01221329704726542
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.282744565217391,
      "likelihood_difference_stderr": 0.1745962513979974,
      "pct_stereotype": 0.4391304347826087,
      "pct_stereotype_stderr": 0.02316441640598207
    },
    "hendrycksTest-marketing": {
      "acc": 0.2948717948717949,
      "acc_stderr": 0.029872577708891145,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03088273697413864
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.19170984455958548,
      "acc_stderr": 0.02840895362624528,
      "acc_norm": 0.2694300518134715,
      "acc_norm_stderr": 0.03201867122877793
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2425531914893617,
      "acc_stderr": 0.028020226271200217,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.026148818018424506
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 6.306009615384616,
      "likelihood_difference_stderr": 0.6323078706267369,
      "pct_stereotype": 0.7230769230769231,
      "pct_stereotype_stderr": 0.055934767585573
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2361111111111111,
      "acc_stderr": 0.028963702570791033,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.030998666304560524
    },
    "piqa": {
      "acc": 0.6947769314472253,
      "acc_stderr": 0.01074426704560648,
      "acc_norm": 0.6871599564744287,
      "acc_norm_stderr": 0.010817714425701083
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.20157967032967,
      "likelihood_difference_stderr": 0.3615253140878653,
      "pct_stereotype": 0.6263736263736264,
      "pct_stereotype_stderr": 0.050993431663867696
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.043300437496507437,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.04668408033024932
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.1901840490797546,
      "acc_stderr": 0.030833491146281252,
      "acc_norm": 0.31901840490797545,
      "acc_norm_stderr": 0.03661997551073836
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "lambada_openai": {
      "ppl": 8.708835222916667,
      "ppl_stderr": 0.2432030860348178,
      "acc": 0.5439549776829031,
      "acc_stderr": 0.006939008354532881
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.21851851851851853,
      "acc_stderr": 0.02519575225182379,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.02794045713622842
    },
    "hendrycksTest-human_aging": {
      "acc": 0.29596412556053814,
      "acc_stderr": 0.03063659134869981,
      "acc_norm": 0.23318385650224216,
      "acc_norm_stderr": 0.028380391147094716
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 2.962152777777778,
      "likelihood_difference_stderr": 0.3152084928606958,
      "pct_stereotype": 0.45555555555555555,
      "pct_stereotype_stderr": 0.052790096466303435
    },
    "hendrycksTest-sociology": {
      "acc": 0.2885572139303483,
      "acc_stderr": 0.03203841040213322,
      "acc_norm": 0.2736318407960199,
      "acc_norm_stderr": 0.03152439186555401
    },
    "hendrycksTest-global_facts": {
      "acc": 0.19,
      "acc_stderr": 0.03942772444036625,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "hendrycksTest-international_law": {
      "acc": 0.21487603305785125,
      "acc_stderr": 0.03749492448709697,
      "acc_norm": 0.4297520661157025,
      "acc_norm_stderr": 0.04519082021319773
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.33587786259541985,
      "acc_stderr": 0.04142313771996663,
      "acc_norm": 0.32061068702290074,
      "acc_norm_stderr": 0.04093329229834277
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 5.0364010989010985,
      "likelihood_difference_stderr": 0.4535330921305476,
      "pct_stereotype": 0.7472527472527473,
      "pct_stereotype_stderr": 0.04580951853732891
    },
    "hendrycksTest-college_biology": {
      "acc": 0.20833333333333334,
      "acc_stderr": 0.03396116205845334,
      "acc_norm": 0.1875,
      "acc_norm_stderr": 0.032639560491693344
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.25177304964539005,
      "acc_stderr": 0.0258921511567094,
      "acc_norm": 0.25886524822695034,
      "acc_norm_stderr": 0.026129572527180848
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.38310546875,
      "likelihood_difference_stderr": 0.16516511294450978,
      "pct_stereotype": 0.609375,
      "pct_stereotype_stderr": 0.02731662195498096
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.6948784722222223,
      "likelihood_difference_stderr": 0.47901853201103023,
      "pct_stereotype": 0.5416666666666666,
      "pct_stereotype_stderr": 0.05913268547421811
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.024562204314142314,
      "acc_norm": 0.22058823529411764,
      "acc_norm_stderr": 0.02518778666022727
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.029620227874790482,
      "acc_norm": 0.29797979797979796,
      "acc_norm_stderr": 0.03258630383836556
    },
    "arc_easy": {
      "acc": 0.5458754208754208,
      "acc_stderr": 0.010216507710244111,
      "acc_norm": 0.4722222222222222,
      "acc_norm_stderr": 0.01024393828588112
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25,
      "acc_stderr": 0.04109974682633932,
      "acc_norm": 0.20535714285714285,
      "acc_norm_stderr": 0.038342410214190735
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.6873641304347826,
      "likelihood_difference_stderr": 0.39076920944340127,
      "pct_stereotype": 0.6,
      "pct_stereotype_stderr": 0.04588314677411235
    },
    "winogrande": {
      "acc": 0.526440410418311,
      "acc_stderr": 0.014032823874407224
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.02564686309713791,
      "acc_norm": 0.369281045751634,
      "acc_norm_stderr": 0.02763417668960266
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768079,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2818181818181818,
      "acc_stderr": 0.043091187099464585,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.040139645540727735
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.932894736842105,
      "likelihood_difference_stderr": 0.23916937227145182,
      "pct_stereotype": 0.6684210526315789,
      "pct_stereotype_stderr": 0.034244247887619504
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2698412698412698,
      "acc_stderr": 0.03970158273235172,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.03970158273235173
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.423980694692904,
      "likelihood_difference_stderr": 0.08571074119584884,
      "pct_stereotype": 0.607036374478235,
      "pct_stereotype_stderr": 0.01193016709674186
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.6225961538461537,
      "likelihood_difference_stderr": 0.6857354754264816,
      "pct_stereotype": 0.38461538461538464,
      "pct_stereotype_stderr": 0.1404416814115811
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2315270935960591,
      "acc_stderr": 0.029678333141444455,
      "acc_norm": 0.2512315270935961,
      "acc_norm_stderr": 0.030516530732694436
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.289649209486166,
      "likelihood_difference_stderr": 0.23633719629249192,
      "pct_stereotype": 0.31620553359683795,
      "pct_stereotype_stderr": 0.029291880485542012
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.20175438596491227,
      "acc_norm_stderr": 0.03775205013583638
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 4.997159090909091,
      "likelihood_difference_stderr": 0.4979048685429528,
      "pct_stereotype": 0.6212121212121212,
      "pct_stereotype_stderr": 0.06016741025240241
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2827586206896552,
      "acc_stderr": 0.03752833958003337,
      "acc_norm": 0.296551724137931,
      "acc_norm_stderr": 0.03806142687309994
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.027553614467863804,
      "acc_norm": 0.3445378151260504,
      "acc_norm_stderr": 0.030868682604121633
    }
  },
  "versions": {
    "logiqa": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-high_school_us_history": 0,
    "crows_pairs_english_autre": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-world_religions": 0,
    "hendrycksTest-high_school_physics": 0,
    "crows_pairs_english_nationality": 0,
    "hendrycksTest-moral_scenarios": 0,
    "arc_challenge": 0,
    "crows_pairs_french_gender": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_world_history": 0,
    "crows_pairs_english_race_color": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-philosophy": 0,
    "crows_pairs_english_physical_appearance": 0,
    "hendrycksTest-college_physics": 0,
    "sciq": 0,
    "hendrycksTest-anatomy": 0,
    "crows_pairs_english_religion": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-professional_psychology": 0,
    "crows_pairs_french_socioeconomic": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-college_computer_science": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-prehistory": 0,
    "wsc": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-high_school_biology": 0,
    "crows_pairs_french": 0,
    "crows_pairs_french_race_color": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-conceptual_physics": 0,
    "crows_pairs_english_disability": 0,
    "hendrycksTest-high_school_statistics": 0,
    "piqa": 0,
    "crows_pairs_english_age": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-college_chemistry": 0,
    "lambada_openai": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-human_aging": 0,
    "crows_pairs_french_age": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-human_sexuality": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-professional_accounting": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_french_physical_appearance": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-high_school_geography": 0,
    "arc_easy": 0,
    "hendrycksTest-machine_learning": 0,
    "crows_pairs_french_religion": 0,
    "winogrande": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-public_relations": 0,
    "crows_pairs_english_socioeconomic": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-formal_logic": 0,
    "crows_pairs_english": 0,
    "crows_pairs_french_autre": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "crows_pairs_french_nationality": 0,
    "hendrycksTest-econometrics": 0,
    "crows_pairs_french_disability": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-high_school_microeconomics": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "use_accelerate=True,pretrained=EleutherAI/pythia-v1.1-1b-bf16,revision=step83000",
    "num_fewshot": 0,
    "batch_size": 32,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}