{
  "results": {
    "piqa": {
      "acc": 0.6430903155603918,
      "acc_stderr": 0.011177909079261195,
      "acc_norm": 0.6273122959738846,
      "acc_norm_stderr": 0.011281318332897748
    },
    "wsc": {
      "acc": 0.34615384615384615,
      "acc_stderr": 0.04687634642174987
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.26455026455026454,
      "acc_stderr": 0.02271746789770861,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.023068188848261117
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.27169811320754716,
      "acc_stderr": 0.027377706624670713,
      "acc_norm": 0.32075471698113206,
      "acc_norm_stderr": 0.028727502957880263
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.17791411042944785,
      "acc_stderr": 0.03004735765580664,
      "acc_norm": 0.3067484662576687,
      "acc_norm_stderr": 0.03623089915724148
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.01567100600933957,
      "acc_norm": 0.26053639846743293,
      "acc_norm_stderr": 0.015696008563807096
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3125,
      "acc_stderr": 0.038760854559127644,
      "acc_norm": 0.2638888888888889,
      "acc_norm_stderr": 0.03685651095897532
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.04133119440243839,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.04643454608906275
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.2967741935483871,
      "acc_stderr": 0.025988500792411894,
      "acc_norm": 0.3032258064516129,
      "acc_norm_stderr": 0.026148685930671746
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322,
      "acc_norm": 0.20175438596491227,
      "acc_norm_stderr": 0.03775205013583638
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.20809248554913296,
      "acc_stderr": 0.030952890217749895,
      "acc_norm": 0.26011560693641617,
      "acc_norm_stderr": 0.03345036916788991
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.25688073394495414,
      "acc_stderr": 0.018732492928342462,
      "acc_norm": 0.24770642201834864,
      "acc_norm_stderr": 0.018508143602547815
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.24468085106382978,
      "acc_stderr": 0.02564555362226673,
      "acc_norm": 0.2375886524822695,
      "acc_norm_stderr": 0.025389512552729906
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 5.849431818181818,
      "likelihood_difference_stderr": 0.5414067097529763,
      "pct_stereotype": 0.3333333333333333,
      "pct_stereotype_stderr": 0.0584705346204686
    },
    "hendrycksTest-security_studies": {
      "acc": 0.31020408163265306,
      "acc_stderr": 0.029613459872484378,
      "acc_norm": 0.24897959183673468,
      "acc_norm_stderr": 0.027682979522960227
    },
    "arc_challenge": {
      "acc": 0.19539249146757678,
      "acc_stderr": 0.01158690718995291,
      "acc_norm": 0.23122866894197952,
      "acc_norm_stderr": 0.012320858834772271
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.2538860103626943,
      "acc_stderr": 0.03141024780565318,
      "acc_norm": 0.2849740932642487,
      "acc_norm_stderr": 0.03257714077709661
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-management": {
      "acc": 0.13592233009708737,
      "acc_stderr": 0.03393295729761012,
      "acc_norm": 0.21359223300970873,
      "acc_norm_stderr": 0.04058042015646035
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2153846153846154,
      "acc_stderr": 0.020843034557462874,
      "acc_norm": 0.2743589743589744,
      "acc_norm_stderr": 0.02262276576749322
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.800407608695652,
      "likelihood_difference_stderr": 0.2026630843555547,
      "pct_stereotype": 0.3630434782608696,
      "pct_stereotype_stderr": 0.022445426974212864
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.394855023852117,
      "likelihood_difference_stderr": 0.09053006408287394,
      "pct_stereotype": 0.5575432319618366,
      "pct_stereotype_stderr": 0.012132147684215487
    },
    "hendrycksTest-astronomy": {
      "acc": 0.2236842105263158,
      "acc_stderr": 0.033911609343436025,
      "acc_norm": 0.3223684210526316,
      "acc_norm_stderr": 0.03803510248351585
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909283
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.851973684210526,
      "likelihood_difference_stderr": 0.2314121849093462,
      "pct_stereotype": 0.6210526315789474,
      "pct_stereotype_stderr": 0.035287650940948406
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.5339673913043477,
      "likelihood_difference_stderr": 0.4306402399027638,
      "pct_stereotype": 0.5478260869565217,
      "pct_stereotype_stderr": 0.04661456979958348
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2630057803468208,
      "acc_stderr": 0.02370309952525817,
      "acc_norm": 0.3092485549132948,
      "acc_norm_stderr": 0.024883140570071755
    },
    "hendrycksTest-human_aging": {
      "acc": 0.336322869955157,
      "acc_stderr": 0.031708824268455,
      "acc_norm": 0.28699551569506726,
      "acc_norm_stderr": 0.030360379710291943
    },
    "hendrycksTest-world_religions": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.03446296217088426,
      "acc_norm": 0.32748538011695905,
      "acc_norm_stderr": 0.035993357714560276
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2052980132450331,
      "acc_stderr": 0.03297986648473836,
      "acc_norm": 0.2185430463576159,
      "acc_norm_stderr": 0.03374235550425694
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.32413793103448274,
      "acc_stderr": 0.03900432069185555,
      "acc_norm": 0.3310344827586207,
      "acc_norm_stderr": 0.039215453124671215
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.23148148148148148,
      "acc_stderr": 0.028765111718046944,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.030851992993257013
    },
    "hendrycksTest-virology": {
      "acc": 0.24096385542168675,
      "acc_stderr": 0.03329394119073531,
      "acc_norm": 0.25301204819277107,
      "acc_norm_stderr": 0.03384429155233136
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.2426470588235294,
      "acc_stderr": 0.02604066247420126,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.026303648393696036
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.29464285714285715,
      "acc_stderr": 0.04327040932578728,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04109974682633932
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.6538461538461537,
      "likelihood_difference_stderr": 1.3041432177804484,
      "pct_stereotype": 0.15384615384615385,
      "pct_stereotype_stderr": 0.10415433852097386
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.857862903225806,
      "likelihood_difference_stderr": 0.5191271689471638,
      "pct_stereotype": 0.8709677419354839,
      "pct_stereotype_stderr": 0.03495073154102977
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 2.964629120879121,
      "likelihood_difference_stderr": 0.29284096446623586,
      "pct_stereotype": 0.4945054945054945,
      "pct_stereotype_stderr": 0.052701445311128796
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.02912652283458682,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.03191178226713545
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.671311936936937,
      "likelihood_difference_stderr": 0.35703231773644273,
      "pct_stereotype": 0.6396396396396397,
      "pct_stereotype_stderr": 0.04577621167070314
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 4.259486607142857,
      "likelihood_difference_stderr": 0.32677801207842877,
      "pct_stereotype": 0.37755102040816324,
      "pct_stereotype_stderr": 0.03471541794449721
    },
    "hendrycksTest-computer_security": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 5.979001976284585,
      "likelihood_difference_stderr": 0.30697130974233605,
      "pct_stereotype": 0.2845849802371542,
      "pct_stereotype_stderr": 0.02842397052208522
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.25980392156862747,
      "acc_stderr": 0.030778554678693268,
      "acc_norm": 0.24019607843137256,
      "acc_norm_stderr": 0.02998373305591361
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 3.580208333333333,
      "likelihood_difference_stderr": 0.4192700300373815,
      "pct_stereotype": 0.3888888888888889,
      "pct_stereotype_stderr": 0.051674686932038624
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 5.610795454545454,
      "likelihood_difference_stderr": 2.0856073161230797,
      "pct_stereotype": 0.5454545454545454,
      "pct_stereotype_stderr": 0.1574591643244434
    },
    "logiqa": {
      "acc": 0.2119815668202765,
      "acc_stderr": 0.016030997960619395,
      "acc_norm": 0.2626728110599078,
      "acc_norm_stderr": 0.017261598347857544
    },
    "winogrande": {
      "acc": 0.4980268350434096,
      "acc_stderr": 0.014052376259225636
    },
    "hendrycksTest-marketing": {
      "acc": 0.3247863247863248,
      "acc_stderr": 0.03067902276549883,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.030882736974138653
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-sociology": {
      "acc": 0.2935323383084577,
      "acc_stderr": 0.03220024104534205,
      "acc_norm": 0.24378109452736318,
      "acc_norm_stderr": 0.03036049015401466
    },
    "hendrycksTest-global_facts": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036846,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.042295258468165065
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.040735243221471276,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.04104947269903394
    },
    "sciq": {
      "acc": 0.729,
      "acc_stderr": 0.014062601350986186,
      "acc_norm": 0.643,
      "acc_norm_stderr": 0.015158521721486774
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.270042194092827,
      "acc_stderr": 0.028900721906293426,
      "acc_norm": 0.29535864978902954,
      "acc_norm_stderr": 0.029696338713422893
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.1658156988188977,
      "likelihood_difference_stderr": 0.15936813730369137,
      "pct_stereotype": 0.4625984251968504,
      "pct_stereotype_stderr": 0.022143566088969842
    },
    "crows_pairs_french": {
      "likelihood_difference": 4.417216383422779,
      "likelihood_difference_stderr": 0.11028760551959911,
      "pct_stereotype": 0.41681574239713776,
      "pct_stereotype_stderr": 0.012043090376959043
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.18719211822660098,
      "acc_stderr": 0.027444924966882618,
      "acc_norm": 0.2512315270935961,
      "acc_norm_stderr": 0.030516530732694436
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2680851063829787,
      "acc_stderr": 0.028957342788342347,
      "acc_norm": 0.20425531914893616,
      "acc_norm_stderr": 0.026355158413349417
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.26633986928104575,
      "acc_stderr": 0.017883188134667192,
      "acc_norm": 0.26143790849673204,
      "acc_norm_stderr": 0.017776947157528037
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.16,
      "acc_stderr": 0.036845294917747094,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.04336432707993179,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364395
    },
    "hendrycksTest-anatomy": {
      "acc": 0.23703703703703705,
      "acc_stderr": 0.03673731683969506,
      "acc_norm": 0.24444444444444444,
      "acc_norm_stderr": 0.037125378336148665
    },
    "arc_easy": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.01019625483869167,
      "acc_norm": 0.3952020202020202,
      "acc_norm_stderr": 0.010031894052790973
    },
    "lambada_openai": {
      "ppl": 32.58271672415868,
      "ppl_stderr": 1.1883315028110615,
      "acc": 0.3475645255191151,
      "acc_stderr": 0.006634353032738008
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.140625,
      "likelihood_difference_stderr": 0.32531258009820135,
      "pct_stereotype": 0.6666666666666666,
      "pct_stereotype_stderr": 0.05594542388644592
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.25210084033613445,
      "acc_stderr": 0.028205545033277726,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.031124619309328177
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 4.860233516483516,
      "likelihood_difference_stderr": 0.5004036595223936,
      "pct_stereotype": 0.7582417582417582,
      "pct_stereotype_stderr": 0.04513082148355002
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.04093601807403326
    },
    "hendrycksTest-philosophy": {
      "acc": 0.19935691318327975,
      "acc_stderr": 0.022691033780549656,
      "acc_norm": 0.24115755627009647,
      "acc_norm_stderr": 0.024296594034763426
    },
    "hendrycksTest-international_law": {
      "acc": 0.1652892561983471,
      "acc_stderr": 0.03390780612972776,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.045454545454545456
    },
    "hendrycksTest-public_relations": {
      "acc": 0.3090909090909091,
      "acc_stderr": 0.044262946482000985,
      "acc_norm": 0.20909090909090908,
      "acc_norm_stderr": 0.03895091015724138
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2875816993464052,
      "acc_stderr": 0.02591780611714716,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.027184498909941616
    },
    "hendrycksTest-professional_law": {
      "acc": 0.24837027379400262,
      "acc_stderr": 0.011035212598034494,
      "acc_norm": 0.27183833116036504,
      "acc_norm_stderr": 0.011363135278651407
    },
    "hendrycksTest-prehistory": {
      "acc": 0.23765432098765432,
      "acc_stderr": 0.023683591837008553,
      "acc_norm": 0.2191358024691358,
      "acc_norm_stderr": 0.023016705640262203
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.24242424242424243,
      "acc_stderr": 0.03346409881055953,
      "acc_norm": 0.3212121212121212,
      "acc_norm_stderr": 0.0364620496325381
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.4029224537037037,
      "likelihood_difference_stderr": 0.25177416697525834,
      "pct_stereotype": 0.4675925925925926,
      "pct_stereotype_stderr": 0.03402801581358966
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3053435114503817,
      "acc_stderr": 0.04039314978724561,
      "acc_norm": 0.2748091603053435,
      "acc_norm_stderr": 0.03915345408847835
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206824
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.15625,
      "likelihood_difference_stderr": 0.6090332035766969,
      "pct_stereotype": 0.6307692307692307,
      "pct_stereotype_stderr": 0.06032456592830046
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 4.03125,
      "likelihood_difference_stderr": 0.21464003280625563,
      "pct_stereotype": 0.48909657320872274,
      "pct_stereotype_stderr": 0.02794420307081864
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.1814814814814815,
      "acc_stderr": 0.02349926466940729,
      "acc_norm": 0.22962962962962963,
      "acc_norm_stderr": 0.025644108639267627
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 5.7421875,
      "likelihood_difference_stderr": 0.6248268998113368,
      "pct_stereotype": 0.5277777777777778,
      "pct_stereotype_stderr": 0.05924743948371486
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.706103515625,
      "likelihood_difference_stderr": 0.19286248269148096,
      "pct_stereotype": 0.590625,
      "pct_stereotype_stderr": 0.027530952052640063
    }
  },
  "versions": {
    "piqa": 0,
    "wsc": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-professional_accounting": 0,
    "crows_pairs_french_disability": 0,
    "hendrycksTest-security_studies": 0,
    "arc_challenge": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "crows_pairs_french_race_color": 0,
    "crows_pairs_english": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-moral_scenarios": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_french_religion": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-world_religions": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-machine_learning": 0,
    "crows_pairs_french_autre": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_english_age": 0,
    "hendrycksTest-high_school_geography": 0,
    "crows_pairs_english_religion": 0,
    "crows_pairs_french_socioeconomic": 0,
    "hendrycksTest-computer_security": 0,
    "crows_pairs_french_nationality": 0,
    "hendrycksTest-high_school_us_history": 0,
    "crows_pairs_french_age": 0,
    "crows_pairs_english_autre": 0,
    "logiqa": 0,
    "winogrande": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-formal_logic": 0,
    "sciq": 0,
    "hendrycksTest-high_school_world_history": 0,
    "crows_pairs_english_race_color": 0,
    "crows_pairs_french": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-anatomy": 0,
    "arc_easy": 0,
    "lambada_openai": 0,
    "crows_pairs_english_physical_appearance": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-high_school_european_history": 0,
    "crows_pairs_english_nationality": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-abstract_algebra": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_gender": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "crows_pairs_french_physical_appearance": 0,
    "crows_pairs_english_gender": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-v1.1-12b-deduped,revision=step3000",
    "num_fewshot": 0,
    "batch_size": 2,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}