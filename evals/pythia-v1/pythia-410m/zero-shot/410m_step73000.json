{
  "results": {
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.2188679245283019,
      "acc_stderr": 0.02544786382510864,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.02825420034443866
    },
    "hendrycksTest-management": {
      "acc": 0.1553398058252427,
      "acc_stderr": 0.03586594738573973,
      "acc_norm": 0.24271844660194175,
      "acc_norm_stderr": 0.04245022486384495
    },
    "hendrycksTest-astronomy": {
      "acc": 0.17763157894736842,
      "acc_stderr": 0.031103182383123394,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.0378272898086547
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.18226600985221675,
      "acc_stderr": 0.02716334085964515,
      "acc_norm": 0.2561576354679803,
      "acc_norm_stderr": 0.030712730070982592
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.2737430167597765,
      "acc_norm_stderr": 0.014912413096372428
    },
    "winogrande": {
      "acc": 0.5248618784530387,
      "acc_stderr": 0.01403510288362775
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.1962962962962963,
      "acc_stderr": 0.02421742132741715,
      "acc_norm": 0.26296296296296295,
      "acc_norm_stderr": 0.02684205787383371
    },
    "hendrycksTest-human_aging": {
      "acc": 0.33183856502242154,
      "acc_stderr": 0.03160295143776679,
      "acc_norm": 0.273542600896861,
      "acc_norm_stderr": 0.029918586707798813
    },
    "logiqa": {
      "acc": 0.19969278033794163,
      "acc_stderr": 0.015680245966420602,
      "acc_norm": 0.271889400921659,
      "acc_norm_stderr": 0.017451716009436832
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2254335260115607,
      "acc_stderr": 0.022497230190967547,
      "acc_norm": 0.3179190751445087,
      "acc_norm_stderr": 0.025070713719153186
    },
    "hendrycksTest-virology": {
      "acc": 0.3373493975903614,
      "acc_stderr": 0.03680783690727581,
      "acc_norm": 0.29518072289156627,
      "acc_norm_stderr": 0.0355092018568963
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.2680288461538463,
      "likelihood_difference_stderr": 0.3083123263316909,
      "pct_stereotype": 0.6153846153846154,
      "pct_stereotype_stderr": 0.05128205128205124
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.524072265625,
      "likelihood_difference_stderr": 0.19354903754015237,
      "pct_stereotype": 0.5625,
      "pct_stereotype_stderr": 0.02777505646718807
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.14185531496063,
      "likelihood_difference_stderr": 0.14549823062738831,
      "pct_stereotype": 0.515748031496063,
      "pct_stereotype_stderr": 0.022194762762659328
    },
    "hendrycksTest-sociology": {
      "acc": 0.2885572139303483,
      "acc_stderr": 0.03203841040213321,
      "acc_norm": 0.2885572139303483,
      "acc_norm_stderr": 0.032038410402133226
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2205128205128205,
      "acc_stderr": 0.02102067268082791,
      "acc_norm": 0.24871794871794872,
      "acc_norm_stderr": 0.0219169577092138
    },
    "arc_easy": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.01025274421743564,
      "acc_norm": 0.4595959595959596,
      "acc_norm_stderr": 0.01022623074088902
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.024954184324879905,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.02718449890994162
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.04350271442923243,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.04350271442923243
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.2132009345794392,
      "likelihood_difference_stderr": 0.18677569577694814,
      "pct_stereotype": 0.5046728971962616,
      "pct_stereotype_stderr": 0.027949629024360143
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.043300437496507437,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.047500773411999854
    },
    "hendrycksTest-international_law": {
      "acc": 0.19008264462809918,
      "acc_stderr": 0.03581796951709282,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.045454545454545456
    },
    "hendrycksTest-philosophy": {
      "acc": 0.19614147909967847,
      "acc_stderr": 0.02255244778047803,
      "acc_norm": 0.31189710610932475,
      "acc_norm_stderr": 0.02631185807185416
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 4.2466119260204085,
      "likelihood_difference_stderr": 0.2983031896916964,
      "pct_stereotype": 0.6020408163265306,
      "pct_stereotype_stderr": 0.0350521715047299
    },
    "piqa": {
      "acc": 0.6681175190424374,
      "acc_stderr": 0.010986617776361594,
      "acc_norm": 0.6686615886833515,
      "acc_norm_stderr": 0.010982077458957339
    },
    "arc_challenge": {
      "acc": 0.19795221843003413,
      "acc_stderr": 0.011643990971573403,
      "acc_norm": 0.2295221843003413,
      "acc_norm_stderr": 0.012288926760890793
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "wsc": {
      "acc": 0.5769230769230769,
      "acc_stderr": 0.04867993747918684
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.26582278481012656,
      "acc_stderr": 0.02875679962965834,
      "acc_norm": 0.24472573839662448,
      "acc_norm_stderr": 0.02798569938703642
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25,
      "acc_stderr": 0.04109974682633932,
      "acc_norm": 0.21428571428571427,
      "acc_norm_stderr": 0.03894641120044792
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.838359980620155,
      "likelihood_difference_stderr": 0.09361635620112221,
      "pct_stereotype": 0.4573643410852713,
      "pct_stereotype_stderr": 0.012168815552485855
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2565359477124183,
      "acc_stderr": 0.017667841612378984,
      "acc_norm": 0.2565359477124183,
      "acc_norm_stderr": 0.017667841612378977
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.31724137931034485,
      "acc_stderr": 0.03878352372138622,
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.03878352372138623
    },
    "sciq": {
      "acc": 0.807,
      "acc_stderr": 0.012486268734370141,
      "acc_norm": 0.709,
      "acc_norm_stderr": 0.01437099598237793
    },
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.8345108695652175,
      "likelihood_difference_stderr": 0.38906893040037516,
      "pct_stereotype": 0.6260869565217392,
      "pct_stereotype_stderr": 0.04531585828644964
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.1514756944444446,
      "likelihood_difference_stderr": 0.29737273963802563,
      "pct_stereotype": 0.6805555555555556,
      "pct_stereotype_stderr": 0.05533504751887218
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.22486772486772486,
      "acc_stderr": 0.021502096078229147,
      "acc_norm": 0.2328042328042328,
      "acc_norm_stderr": 0.02176596167215453
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.04142439719489363,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.23897058823529413,
      "acc_stderr": 0.025905280644893006,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.025767252010855952
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 5.34375,
      "likelihood_difference_stderr": 0.6133250765207491,
      "pct_stereotype": 0.6461538461538462,
      "pct_stereotype_stderr": 0.05977027026123099
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.7259615384615383,
      "likelihood_difference_stderr": 1.4761566260028478,
      "pct_stereotype": 0.5384615384615384,
      "pct_stereotype_stderr": 0.14390989949130545
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.31063829787234043,
      "acc_stderr": 0.03025123757921317,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.026148818018424506
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.03192271569548299,
      "acc_norm": 0.23030303030303031,
      "acc_norm_stderr": 0.03287666758603489
    },
    "hendrycksTest-anatomy": {
      "acc": 0.18518518518518517,
      "acc_stderr": 0.03355677216313142,
      "acc_norm": 0.23703703703703705,
      "acc_norm_stderr": 0.03673731683969506
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.2541507024265645,
      "acc_stderr": 0.015569254692045774,
      "acc_norm": 0.2669220945083014,
      "acc_norm_stderr": 0.015818450894777545
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 5.620738636363637,
      "likelihood_difference_stderr": 2.015520490275413,
      "pct_stereotype": 0.7272727272727273,
      "pct_stereotype_stderr": 0.14083575804390605
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.2331288343558282,
      "acc_stderr": 0.03322015795776741,
      "acc_norm": 0.27607361963190186,
      "acc_norm_stderr": 0.03512385283705051
    },
    "hendrycksTest-marketing": {
      "acc": 0.27350427350427353,
      "acc_stderr": 0.02920254015343115,
      "acc_norm": 0.32051282051282054,
      "acc_norm_stderr": 0.030572811310299607
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2375886524822695,
      "acc_stderr": 0.025389512552729906,
      "acc_norm": 0.22340425531914893,
      "acc_norm_stderr": 0.024847921358063962
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.18134715025906736,
      "acc_stderr": 0.02780703236068609,
      "acc_norm": 0.27461139896373055,
      "acc_norm_stderr": 0.03221024508041154
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 5.430871212121212,
      "likelihood_difference_stderr": 0.5911481897402469,
      "pct_stereotype": 0.5151515151515151,
      "pct_stereotype_stderr": 0.06198888629778894
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.2696078431372549,
      "acc_stderr": 0.03114557065948678,
      "acc_norm": 0.25980392156862747,
      "acc_norm_stderr": 0.03077855467869326
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.21717171717171718,
      "acc_stderr": 0.02937661648494563,
      "acc_norm": 0.25757575757575757,
      "acc_norm_stderr": 0.03115626951964683
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 4.084478021978022,
      "likelihood_difference_stderr": 0.33012438286209433,
      "pct_stereotype": 0.7142857142857143,
      "pct_stereotype_stderr": 0.04761904761904759
    },
    "hendrycksTest-college_biology": {
      "acc": 0.22916666666666666,
      "acc_stderr": 0.03514697467862388,
      "acc_norm": 0.2152777777777778,
      "acc_norm_stderr": 0.034370793441061344
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.029157522184605617,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.029157522184605617
    },
    "lambada_openai": {
      "ppl": 13.009469357175831,
      "ppl_stderr": 0.39906607296262636,
      "acc": 0.47739181059576946,
      "acc_stderr": 0.0069588529701874
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.263454861111111,
      "likelihood_difference_stderr": 0.23509744376345895,
      "pct_stereotype": 0.49074074074074076,
      "pct_stereotype_stderr": 0.03409386946992699
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.447282608695652,
      "likelihood_difference_stderr": 0.1696726597091016,
      "pct_stereotype": 0.358695652173913,
      "pct_stereotype_stderr": 0.02238663434141095
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237654,
      "acc_norm": 0.18627450980392157,
      "acc_norm_stderr": 0.038739587141493545
    },
    "hendrycksTest-prehistory": {
      "acc": 0.2623456790123457,
      "acc_stderr": 0.024477222856135114,
      "acc_norm": 0.21604938271604937,
      "acc_norm_stderr": 0.022899162918445806
    },
    "hendrycksTest-global_facts": {
      "acc": 0.18,
      "acc_stderr": 0.038612291966536934,
      "acc_norm": 0.17,
      "acc_norm_stderr": 0.03775251680686371
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.59375,
      "likelihood_difference_stderr": 0.5017351927424835,
      "pct_stereotype": 0.7526881720430108,
      "pct_stereotype_stderr": 0.0449817218566707
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.1791907514450867,
      "acc_stderr": 0.029242513059063294,
      "acc_norm": 0.2658959537572254,
      "acc_norm_stderr": 0.033687629322594295
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3435114503816794,
      "acc_stderr": 0.041649760719448786,
      "acc_norm": 0.31297709923664124,
      "acc_norm_stderr": 0.04066962905677697
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 4.195138888888889,
      "likelihood_difference_stderr": 0.40626539846186366,
      "pct_stereotype": 0.35555555555555557,
      "pct_stereotype_stderr": 0.05074011803597719
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2542372881355932,
      "acc_stderr": 0.011121129007840664,
      "acc_norm": 0.303129074315515,
      "acc_norm_stderr": 0.011738669951254286
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 4.3046875,
      "likelihood_difference_stderr": 0.561472320628292,
      "pct_stereotype": 0.5694444444444444,
      "pct_stereotype_stderr": 0.05876396677084613
    },
    "hendrycksTest-world_religions": {
      "acc": 0.29239766081871343,
      "acc_stderr": 0.034886477134579215,
      "acc_norm": 0.34502923976608185,
      "acc_norm_stderr": 0.03645981377388807
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.24503311258278146,
      "acc_stderr": 0.035118075718047245,
      "acc_norm": 0.26490066225165565,
      "acc_norm_stderr": 0.03603038545360384
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.04040610178208841,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.04134913018303316
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 4.048519736842105,
      "likelihood_difference_stderr": 0.24077103436162284,
      "pct_stereotype": 0.6105263157894737,
      "pct_stereotype_stderr": 0.03546993163737159
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.3203171586165774,
      "likelihood_difference_stderr": 0.08723326294822696,
      "pct_stereotype": 0.5754323196183662,
      "pct_stereotype_stderr": 0.012073511044765938
    },
    "hendrycksTest-computer_security": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.23548387096774193,
      "acc_stderr": 0.024137632429337717,
      "acc_norm": 0.2806451612903226,
      "acc_norm_stderr": 0.025560604721022884
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542129
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-security_studies": {
      "acc": 0.30612244897959184,
      "acc_stderr": 0.029504896454595964,
      "acc_norm": 0.27755102040816326,
      "acc_norm_stderr": 0.028666857790274648
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.270256916996048,
      "likelihood_difference_stderr": 0.2324218385830787,
      "pct_stereotype": 0.28063241106719367,
      "pct_stereotype_stderr": 0.028303756335890395
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.21428571428571427,
      "acc_stderr": 0.0266535315967155,
      "acc_norm": 0.29831932773109243,
      "acc_norm_stderr": 0.02971914287634286
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.20733944954128442,
      "acc_stderr": 0.017381415563608664,
      "acc_norm": 0.23669724770642203,
      "acc_norm_stderr": 0.01822407811729908
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 2.969313063063063,
      "likelihood_difference_stderr": 0.3206415342458023,
      "pct_stereotype": 0.6846846846846847,
      "pct_stereotype_stderr": 0.04430181273152669
    }
  },
  "versions": {
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-moral_scenarios": 0,
    "winogrande": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-human_aging": 0,
    "logiqa": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-virology": 0,
    "crows_pairs_english_age": 0,
    "hendrycksTest-medical_genetics": 0,
    "crows_pairs_english_gender": 0,
    "crows_pairs_english_race_color": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "arc_easy": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-public_relations": 0,
    "crows_pairs_french_gender": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-business_ethics": 0,
    "crows_pairs_french_socioeconomic": 0,
    "piqa": 0,
    "arc_challenge": 0,
    "hendrycksTest-college_computer_science": 0,
    "wsc": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-machine_learning": 0,
    "crows_pairs_french": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-electrical_engineering": 0,
    "sciq": 0,
    "crows_pairs_french_religion": 0,
    "crows_pairs_english_physical_appearance": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-professional_medicine": 0,
    "crows_pairs_english_disability": 0,
    "crows_pairs_french_autre": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-miscellaneous": 0,
    "crows_pairs_english_autre": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "crows_pairs_french_disability": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-high_school_geography": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-high_school_statistics": 0,
    "lambada_openai": 0,
    "crows_pairs_english_nationality": 0,
    "crows_pairs_french_race_color": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-global_facts": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-human_sexuality": 0,
    "crows_pairs_french_age": 0,
    "hendrycksTest-professional_law": 0,
    "crows_pairs_french_physical_appearance": 0,
    "hendrycksTest-world_religions": 0,
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-formal_logic": 0,
    "crows_pairs_english_socioeconomic": 0,
    "crows_pairs_english": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-security_studies": 0,
    "crows_pairs_french_nationality": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "crows_pairs_english_religion": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "pretrained=EleutherAI/pythia-v1.1-410m,revision=step73000",
    "num_fewshot": 0,
    "batch_size": 16,
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}