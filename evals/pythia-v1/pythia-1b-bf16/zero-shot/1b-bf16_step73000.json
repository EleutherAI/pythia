{
  "results": {
    "crows_pairs_french_religion": {
      "likelihood_difference": 3.600271739130435,
      "likelihood_difference_stderr": 0.38232529886102556,
      "pct_stereotype": 0.6086956521739131,
      "pct_stereotype_stderr": 0.04570934635111713
    },
    "crows_pairs_french_socioeconomic": {
      "likelihood_difference": 4.017697704081633,
      "likelihood_difference_stderr": 0.31660988455192435,
      "pct_stereotype": 0.6020408163265306,
      "pct_stereotype_stderr": 0.0350521715047299
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.24848484848484848,
      "acc_stderr": 0.033744026441394036,
      "acc_norm": 0.28484848484848485,
      "acc_norm_stderr": 0.035243908445117836
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.2147239263803681,
      "acc_stderr": 0.03226219377286774,
      "acc_norm": 0.3006134969325153,
      "acc_norm_stderr": 0.03602511318806771
    },
    "crows_pairs_french_physical_appearance": {
      "likelihood_difference": 3.7078993055555554,
      "likelihood_difference_stderr": 0.4701454352205265,
      "pct_stereotype": 0.5277777777777778,
      "pct_stereotype_stderr": 0.05924743948371487
    },
    "wsc": {
      "acc": 0.36538461538461536,
      "acc_stderr": 0.0474473339327792
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2636363636363636,
      "acc_stderr": 0.04220224692971987,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.03831305140884601
    },
    "arc_easy": {
      "acc": 0.5484006734006734,
      "acc_stderr": 0.010211600726405224,
      "acc_norm": 0.4877946127946128,
      "acc_norm_stderr": 0.010256726235129009
    },
    "crows_pairs_french": {
      "likelihood_difference": 3.7027523106738225,
      "likelihood_difference_stderr": 0.09497422616863836,
      "pct_stereotype": 0.46332737030411447,
      "pct_stereotype_stderr": 0.012180404031943273
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.24789915966386555,
      "acc_stderr": 0.028047967224176892,
      "acc_norm": 0.3277310924369748,
      "acc_norm_stderr": 0.030489911417673227
    },
    "crows_pairs_english_religion": {
      "likelihood_difference": 3.2419763513513513,
      "likelihood_difference_stderr": 0.33768521167191184,
      "pct_stereotype": 0.7117117117117117,
      "pct_stereotype_stderr": 0.04318860867532051
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2553191489361702,
      "acc_stderr": 0.026011992930902013,
      "acc_norm": 0.24468085106382978,
      "acc_norm_stderr": 0.025645553622266726
    },
    "hendrycksTest-nutrition": {
      "acc": 0.28104575163398693,
      "acc_stderr": 0.025738854797818716,
      "acc_norm": 0.3627450980392157,
      "acc_norm_stderr": 0.027530078447110303
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.18,
      "acc_stderr": 0.03861229196653694,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.22321428571428573,
      "acc_stderr": 0.039523019677025116,
      "acc_norm": 0.22321428571428573,
      "acc_norm_stderr": 0.03952301967702511
    },
    "crows_pairs_english_autre": {
      "likelihood_difference": 5.181818181818182,
      "likelihood_difference_stderr": 2.031603195612147,
      "pct_stereotype": 0.5454545454545454,
      "pct_stereotype_stderr": 0.1574591643244434
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24804469273743016,
      "acc_stderr": 0.014444157808261427,
      "acc_norm": 0.26927374301675977,
      "acc_norm_stderr": 0.014835616582882578
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.18382352941176472,
      "acc_stderr": 0.023529242185193106,
      "acc_norm": 0.23161764705882354,
      "acc_norm_stderr": 0.025626533803777562
    },
    "hendrycksTest-astronomy": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.03317672787533157,
      "acc_norm": 0.34868421052631576,
      "acc_norm_stderr": 0.0387813988879761
    },
    "crows_pairs_english_age": {
      "likelihood_difference": 3.2747252747252746,
      "likelihood_difference_stderr": 0.3519306693765792,
      "pct_stereotype": 0.6153846153846154,
      "pct_stereotype_stderr": 0.05128205128205124
    },
    "crows_pairs_french_age": {
      "likelihood_difference": 3.3854166666666665,
      "likelihood_difference_stderr": 0.3174252937363695,
      "pct_stereotype": 0.4777777777777778,
      "pct_stereotype_stderr": 0.05294752255076824
    },
    "hendrycksTest-college_biology": {
      "acc": 0.25,
      "acc_stderr": 0.03621034121889507,
      "acc_norm": 0.22916666666666666,
      "acc_norm_stderr": 0.035146974678623884
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.23121387283236994,
      "acc_stderr": 0.0321473730202947,
      "acc_norm": 0.24277456647398843,
      "acc_norm_stderr": 0.0326926380614177
    },
    "arc_challenge": {
      "acc": 0.2431740614334471,
      "acc_stderr": 0.012536554144587092,
      "acc_norm": 0.26535836177474403,
      "acc_norm_stderr": 0.012902554762313967
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2037037037037037,
      "acc_stderr": 0.027467401804057982,
      "acc_norm": 0.26851851851851855,
      "acc_norm_stderr": 0.030225226160012393
    },
    "lambada_openai": {
      "ppl": 8.77783217454448,
      "ppl_stderr": 0.2433317001646285,
      "acc": 0.5449252862410247,
      "acc_stderr": 0.0069378020701983565
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.25738396624472576,
      "acc_stderr": 0.028458820991460295,
      "acc_norm": 0.27848101265822783,
      "acc_norm_stderr": 0.02917868230484255
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "crows_pairs_english": {
      "likelihood_difference": 3.4093712731067383,
      "likelihood_difference_stderr": 0.08691118651555843,
      "pct_stereotype": 0.5909361955873583,
      "pct_stereotype_stderr": 0.012009607538515814
    },
    "piqa": {
      "acc": 0.690424374319913,
      "acc_stderr": 0.010786656752183344,
      "acc_norm": 0.6964091403699674,
      "acc_norm_stderr": 0.01072807989307637
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-philosophy": {
      "acc": 0.22186495176848875,
      "acc_stderr": 0.02359885829286305,
      "acc_norm": 0.3054662379421222,
      "acc_norm_stderr": 0.026160584450140488
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2413793103448276,
      "acc_stderr": 0.03565998174135302,
      "acc_norm": 0.2482758620689655,
      "acc_norm_stderr": 0.03600105692727771
    },
    "crows_pairs_french_gender": {
      "likelihood_difference": 3.378261292834891,
      "likelihood_difference_stderr": 0.18192101543275954,
      "pct_stereotype": 0.5358255451713395,
      "pct_stereotype_stderr": 0.02787900925837708
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.029126522834586825,
      "acc_norm": 0.29292929292929293,
      "acc_norm_stderr": 0.03242497958178815
    },
    "hendrycksTest-professional_law": {
      "acc": 0.25554106910039115,
      "acc_stderr": 0.0111398578335985,
      "acc_norm": 0.29465449804432853,
      "acc_norm_stderr": 0.011643576764069559
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.24150943396226415,
      "acc_stderr": 0.026341480371118362,
      "acc_norm": 0.32452830188679244,
      "acc_norm_stderr": 0.028815615713432118
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2512820512820513,
      "acc_stderr": 0.021992016662370557,
      "acc_norm": 0.23846153846153847,
      "acc_norm_stderr": 0.021606294494647727
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.25163398692810457,
      "acc_stderr": 0.01755581809132226,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.018120224251484587
    },
    "hendrycksTest-sociology": {
      "acc": 0.30845771144278605,
      "acc_stderr": 0.032658195885126966,
      "acc_norm": 0.2537313432835821,
      "acc_norm_stderr": 0.030769444967296014
    },
    "crows_pairs_english_nationality": {
      "likelihood_difference": 3.427951388888889,
      "likelihood_difference_stderr": 0.23119272421630685,
      "pct_stereotype": 0.5324074074074074,
      "pct_stereotype_stderr": 0.03402801581358966
    },
    "hendrycksTest-computer_security": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "crows_pairs_english_sexual_orientation": {
      "likelihood_difference": 4.926075268817204,
      "likelihood_difference_stderr": 0.5051158553084589,
      "pct_stereotype": 0.8172043010752689,
      "pct_stereotype_stderr": 0.04029530010615518
    },
    "crows_pairs_english_race_color": {
      "likelihood_difference": 3.3182824803149606,
      "likelihood_difference_stderr": 0.15009338920294082,
      "pct_stereotype": 0.5433070866141733,
      "pct_stereotype_stderr": 0.022122328731374527
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.20689655172413793,
      "acc_stderr": 0.028501378167893946,
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.03010833071801162
    },
    "hendrycksTest-marketing": {
      "acc": 0.2905982905982906,
      "acc_stderr": 0.02974504857267404,
      "acc_norm": 0.32905982905982906,
      "acc_norm_stderr": 0.03078232157768816
    },
    "hendrycksTest-management": {
      "acc": 0.1941747572815534,
      "acc_stderr": 0.03916667762822586,
      "acc_norm": 0.24271844660194175,
      "acc_norm_stderr": 0.04245022486384493
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3511450381679389,
      "acc_stderr": 0.04186445163013751,
      "acc_norm": 0.3282442748091603,
      "acc_norm_stderr": 0.04118438565806298
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3273542600896861,
      "acc_stderr": 0.03149384670994131,
      "acc_norm": 0.273542600896861,
      "acc_norm_stderr": 0.029918586707798827
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768079,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.22486772486772486,
      "acc_stderr": 0.021502096078229147,
      "acc_norm": 0.25132275132275134,
      "acc_norm_stderr": 0.022340482339643895
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.22201834862385322,
      "acc_stderr": 0.01781884956479663,
      "acc_norm": 0.23302752293577983,
      "acc_norm_stderr": 0.018125669180861483
    },
    "crows_pairs_french_disability": {
      "likelihood_difference": 5.113162878787879,
      "likelihood_difference_stderr": 0.5847428877460528,
      "pct_stereotype": 0.5909090909090909,
      "pct_stereotype_stderr": 0.06098367211363066
    },
    "crows_pairs_english_physical_appearance": {
      "likelihood_difference": 3.4479166666666665,
      "likelihood_difference_stderr": 0.3453567601964385,
      "pct_stereotype": 0.7083333333333334,
      "pct_stereotype_stderr": 0.05394274771736147
    },
    "crows_pairs_english_socioeconomic": {
      "likelihood_difference": 3.7135690789473683,
      "likelihood_difference_stderr": 0.24915390164416512,
      "pct_stereotype": 0.5842105263157895,
      "pct_stereotype_stderr": 0.0358501132552001
    },
    "hendrycksTest-college_physics": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.04023382273617747,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.042207736591714534
    },
    "hendrycksTest-prehistory": {
      "acc": 0.2345679012345679,
      "acc_stderr": 0.023576881744005716,
      "acc_norm": 0.21296296296296297,
      "acc_norm_stderr": 0.022779719088733396
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.04006168083848875,
      "acc_norm": 0.29365079365079366,
      "acc_norm_stderr": 0.04073524322147126
    },
    "crows_pairs_english_disability": {
      "likelihood_difference": 6.1134615384615385,
      "likelihood_difference_stderr": 0.5738577813095281,
      "pct_stereotype": 0.676923076923077,
      "pct_stereotype_stderr": 0.05845647751373333
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3216374269005848,
      "acc_stderr": 0.03582529442573122,
      "acc_norm": 0.3567251461988304,
      "acc_norm_stderr": 0.03674013002860954
    },
    "crows_pairs_french_nationality": {
      "likelihood_difference": 4.183670948616601,
      "likelihood_difference_stderr": 0.24079224411069233,
      "pct_stereotype": 0.308300395256917,
      "pct_stereotype_stderr": 0.02909012143059231
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.26382978723404255,
      "acc_stderr": 0.028809989854102973,
      "acc_norm": 0.22127659574468084,
      "acc_norm_stderr": 0.027136349602424052
    },
    "hendrycksTest-virology": {
      "acc": 0.3132530120481928,
      "acc_stderr": 0.03610805018031023,
      "acc_norm": 0.2710843373493976,
      "acc_norm_stderr": 0.034605799075530276
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.2161290322580645,
      "acc_stderr": 0.02341529343356853,
      "acc_norm": 0.24838709677419354,
      "acc_norm_stderr": 0.024580028921481006
    },
    "hendrycksTest-anatomy": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.035914440841969694,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.034554737023254366
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2947976878612717,
      "acc_stderr": 0.02454761779480383,
      "acc_norm": 0.32947976878612717,
      "acc_norm_stderr": 0.02530525813187971
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.23039215686274508,
      "acc_stderr": 0.029554292605695066,
      "acc_norm": 0.28921568627450983,
      "acc_norm_stderr": 0.03182231867647555
    },
    "hendrycksTest-security_studies": {
      "acc": 0.32653061224489793,
      "acc_stderr": 0.030021056238440313,
      "acc_norm": 0.2693877551020408,
      "acc_norm_stderr": 0.02840125202902294
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.0414243971948936,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.041424397194893624
    },
    "winogrande": {
      "acc": 0.5122336227308603,
      "acc_stderr": 0.014048278820405616
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2582781456953642,
      "acc_stderr": 0.035737053147634576,
      "acc_norm": 0.24503311258278146,
      "acc_norm_stderr": 0.03511807571804725
    },
    "crows_pairs_english_gender": {
      "likelihood_difference": 2.397509765625,
      "likelihood_difference_stderr": 0.16006084917283678,
      "pct_stereotype": 0.553125,
      "pct_stereotype_stderr": 0.027836160509246814
    },
    "hendrycksTest-international_law": {
      "acc": 0.19008264462809918,
      "acc_stderr": 0.03581796951709282,
      "acc_norm": 0.4462809917355372,
      "acc_norm_stderr": 0.0453793517794788
    },
    "crows_pairs_french_sexual_orientation": {
      "likelihood_difference": 4.2458791208791204,
      "likelihood_difference_stderr": 0.4099670891106825,
      "pct_stereotype": 0.6153846153846154,
      "pct_stereotype_stderr": 0.051282051282051246
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.17098445595854922,
      "acc_stderr": 0.02717121368316453,
      "acc_norm": 0.23834196891191708,
      "acc_norm_stderr": 0.030748905363909895
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036845,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "logiqa": {
      "acc": 0.2350230414746544,
      "acc_stderr": 0.01663116682389096,
      "acc_norm": 0.30261136712749614,
      "acc_norm_stderr": 0.018018696598158843
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.21851851851851853,
      "acc_stderr": 0.025195752251823786,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.02840653309060846
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.2515964240102171,
      "acc_stderr": 0.015517322365529614,
      "acc_norm": 0.25798212005108556,
      "acc_norm_stderr": 0.01564583018834895
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.28703703703703703,
      "acc_stderr": 0.04373313040914761,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.04643454608906274
    },
    "hendrycksTest-global_facts": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036846,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "crows_pairs_french_autre": {
      "likelihood_difference": 3.766826923076923,
      "likelihood_difference_stderr": 0.8995080744213534,
      "pct_stereotype": 0.5384615384615384,
      "pct_stereotype_stderr": 0.14390989949130545
    },
    "sciq": {
      "acc": 0.835,
      "acc_stderr": 0.011743632866916154,
      "acc_norm": 0.757,
      "acc_norm_stderr": 0.013569640199177451
    },
    "crows_pairs_french_race_color": {
      "likelihood_difference": 3.305774456521739,
      "likelihood_difference_stderr": 0.18529473726659748,
      "pct_stereotype": 0.3391304347826087,
      "pct_stereotype_stderr": 0.02209708145176117
    }
  },
  "versions": {
    "crows_pairs_french_religion": 0,
    "crows_pairs_french_socioeconomic": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-logical_fallacies": 0,
    "crows_pairs_french_physical_appearance": 0,
    "wsc": 0,
    "hendrycksTest-public_relations": 0,
    "arc_easy": 0,
    "crows_pairs_french": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "crows_pairs_english_religion": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-machine_learning": 0,
    "crows_pairs_english_autre": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-astronomy": 0,
    "crows_pairs_english_age": 0,
    "crows_pairs_french_age": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_medicine": 0,
    "arc_challenge": 0,
    "hendrycksTest-high_school_statistics": 0,
    "lambada_openai": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "crows_pairs_english": 0,
    "piqa": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-electrical_engineering": 0,
    "crows_pairs_french_gender": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-sociology": 0,
    "crows_pairs_english_nationality": 0,
    "hendrycksTest-computer_security": 0,
    "crows_pairs_english_sexual_orientation": 0,
    "crows_pairs_english_race_color": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-high_school_psychology": 0,
    "crows_pairs_french_disability": 0,
    "crows_pairs_english_physical_appearance": 0,
    "crows_pairs_english_socioeconomic": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-formal_logic": 0,
    "crows_pairs_english_disability": 0,
    "hendrycksTest-world_religions": 0,
    "crows_pairs_french_nationality": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-econometrics": 0,
    "winogrande": 0,
    "hendrycksTest-high_school_physics": 0,
    "crows_pairs_english_gender": 0,
    "hendrycksTest-international_law": 0,
    "crows_pairs_french_sexual_orientation": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-medical_genetics": 0,
    "logiqa": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-global_facts": 0,
    "crows_pairs_french_autre": 0,
    "sciq": 0,
    "crows_pairs_french_race_color": 0
  },
  "config": {
    "model": "hf-causal",
    "model_args": "use_accelerate=True,pretrained=EleutherAI/pythia-v1.1-1b-bf16,revision=step73000",
    "num_fewshot": 0,
    "batch_size": 32,
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}